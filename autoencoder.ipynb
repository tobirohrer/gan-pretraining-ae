{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import os, os.path\n",
    "#import tensorflow_addons as tfa\n",
    "import math\n",
    "import json\n",
    "from statistics import median\n",
    "import sklearn.metrics\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "IMG_CHANNELS = 1\n",
    "\n",
    "VALIDATION_SET_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "PATCH_SIZE = 64\n",
    "\n",
    "PATH_TRAINING = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create shuffeled TF-Datasets containing the paths to the files\n",
    "def shuffle_paths(ds_paths, shuffle_loops):\n",
    "    #see https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle/48096625#48096625\n",
    "    list_paths = list(ds_paths.as_numpy_iterator())\n",
    "    for i in range(shuffle_loops):\n",
    "        random.seed(4)\n",
    "        random.shuffle(list_paths)\n",
    "    return tf.data.Dataset.from_tensor_slices(list_paths)\n",
    "\n",
    "#train paths only contain live fingerprints\n",
    "ds_train_paths = tf.data.Dataset.list_files(str(PATH_TRAINING + '*/Live/*.png'), seed=4)\n",
    "ds_train_paths = shuffle_paths(ds_train_paths,10)\n",
    "\n",
    "ds_fake_validation_paths = tf.data.Dataset.list_files(str(PATH_TRAINING + '*/Fake/*.png'), seed=4)\n",
    "ds_fake_validation_paths = shuffle_paths(ds_fake_validation_paths,10)\n",
    "ds_fake_validation_paths = ds_fake_validation_paths.take(int(VALIDATION_SET_SIZE/2))\n",
    "\n",
    "ds_live_validation_paths = ds_train_paths.take(int(VALIDATION_SET_SIZE/2))\n",
    "ds_train_paths = ds_train_paths.skip(int(VALIDATION_SET_SIZE/2))\n",
    "\n",
    "ds_validation_paths = ds_fake_validation_paths.concatenate(ds_live_validation_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    if parts[-2] == 'Live':\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def decode_bmp(file_path):\n",
    "    file = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_bmp(file, channels=1)\n",
    "    return img    \n",
    "\n",
    "def decode_png(file_path):\n",
    "    file = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_png(file, channels=1)\n",
    "    return img\n",
    "\n",
    "def equalize_hist(img):\n",
    "    img_zero_map = (img != 255);\n",
    "    hist, bins = np.histogram(img[img_zero_map], 256,[0,255])\n",
    "    cdf = hist.cumsum()\n",
    "    cdf = (cdf - cdf.min())*255/(cdf.max()-cdf.min())\n",
    "    cdf = np.ma.filled(cdf, 255).astype('uint8')\n",
    "    return cdf[img]\n",
    "\n",
    "def exctract_roi(img):\n",
    "    \n",
    "    img_height = img.shape[0]\n",
    "    img_width = img.shape[1]\n",
    "\n",
    "    y_start = 0\n",
    "    y_stop = img_height\n",
    "    x_start = 0\n",
    "    x_stop = img_width\n",
    "    \n",
    "    for i in range(img_height):\n",
    "        if tf.math.reduce_sum(img[i], axis=None, keepdims=False, name=None) != img_width:\n",
    "            y_start = i\n",
    "            break\n",
    "\n",
    "    for i in range(img_height-1, 0, -1):\n",
    "        if tf.math.reduce_sum(img[i], axis=None, keepdims=False, name=None) != img_width:\n",
    "            y_stop = i\n",
    "            break\n",
    "\n",
    "    for i in range(img_width):\n",
    "        if tf.math.reduce_sum(img[:,i], axis=None, keepdims=False, name=None) != img_height:\n",
    "            x_start = i\n",
    "            break\n",
    "\n",
    "    for i in range(img_width-1, 0, -1):\n",
    "        if tf.math.reduce_sum(img[:,i], axis=None, keepdims=False, name=None) != img_height:\n",
    "            x_stop = i\n",
    "            break\n",
    "\n",
    "    img = img[y_start:y_stop,x_start:x_stop]\n",
    "    img = (img-1)*-1\n",
    "    \n",
    "    if FULL_IMAGE:\n",
    "        img = tf.image.resize_with_pad(img, IMG_HEIGHT, IMG_WIDTH)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def get_random_patch(image):\n",
    "    non_zero_count = 0\n",
    "    loop_count = 0\n",
    "    #making sure the crop does not contain mainly void.\n",
    "    while non_zero_count < 1100:\n",
    "        loop_count += 1\n",
    "        cropped = tf.image.random_crop(image, [PATCH_SIZE, PATCH_SIZE,IMG_CHANNELS], seed=None, name=None)\n",
    "        #cropped = tf.image.resize(cropped, [IMG_HEIGHT, IMG_WIDTH])\n",
    "        non_zero_count = tf.math.count_nonzero(cropped)\n",
    "        if loop_count > 10:\n",
    "            return cropped\n",
    "    return cropped\n",
    "\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    img = decode_png(file_path)\n",
    "    \n",
    "    #normalize img data to [0,1) scale\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    \n",
    "    img = exctract_roi(img)\n",
    "\n",
    "    #for transfer learning which nets which are trained on RGB imges\n",
    "    if IMG_CHANNELS == 3:\n",
    "        img = tf.concat([img,img,img], 2)\n",
    "        \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define mappable functions to run custom python code in tf\n",
    "\n",
    "def mappable_get_random_patch(image,label):\n",
    "    random_patch = tf.py_function(func=get_random_patch,\n",
    "                                inp=[image],\n",
    "                                Tout=(tf.float32))\n",
    "    result_tensor = random_patch, label\n",
    "    result_tensor[0].set_shape((PATCH_SIZE, PATCH_SIZE, IMG_CHANNELS))\n",
    "    result_tensor[1].set_shape(())\n",
    "    return result_tensor\n",
    "\n",
    "def mappable_fn(x):\n",
    "    result_tensor = tf.py_function(func=process_path,\n",
    "                                inp=[x],\n",
    "                                Tout=(tf.float32,tf.uint8))\n",
    "    result_tensor[0].set_shape((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    result_tensor[1].set_shape(())\n",
    "    return result_tensor\n",
    "\n",
    "def mappable_fn_patch(x):\n",
    "    result_tensor = tf.py_function(func=process_path,\n",
    "                                inp=[x],\n",
    "                                Tout=(tf.float32,tf.uint8))\n",
    "    return result_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def augment(image,label):\n",
    "    image = tf.image.random_flip_left_right(image, seed=None)\n",
    "    \n",
    "    #degrees = tf.random.uniform([], minval=-20, maxval=20, dtype=tf.dtypes.float32, seed=None, name=None)\n",
    "    #image = tfa.image.transform_ops.rotate(image, degrees * math.pi/180)\n",
    "    \n",
    "    brightness = tf.random.uniform([], minval=0.75, maxval=1.25, dtype=tf.dtypes.float32, seed=None, name=None)\n",
    "    image = image*brightness\n",
    "    image = tf.clip_by_value(image, 0, 1, name=None)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def center_crop_validation(img, label):\n",
    "    #This is necessary, because augmentation during training\n",
    "    croped_img = tf.image.central_crop(img, 0.90) #200/224\n",
    "    resized_img = tf.image.resize_with_pad(croped_img, IMG_HEIGHT, IMG_WIDTH)\n",
    "    return resized_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def label_is_train_data(img, label):\n",
    "    return img, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gan_rescale(image, label):\n",
    "    return image*2-1, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_patch_train = (ds_train_paths\n",
    "            .map(mappable_fn_patch, num_parallel_calls=AUTOTUNE)\n",
    "            .cache()\n",
    "            .shuffle(buffer_size=4000)\n",
    "            .map(augment, num_parallel_calls=AUTOTUNE) # randomizes the image based on augmentation rules\n",
    "            .map(mappable_get_random_patch)\n",
    "            .map(gan_rescale)\n",
    "            .map(label_is_train_data)\n",
    "            .batch(BATCH_SIZE, drop_remainder=True)\n",
    "            .prefetch(buffer_size=AUTOTUNE)\n",
    "           )\n",
    "\n",
    "ds_patch_validation = (ds_validation_paths\n",
    "                 .map(mappable_fn_patch, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .map(mappable_get_random_patch)\n",
    "                 .map(gan_rescale)\n",
    "                 .map(label_is_train_data)  \n",
    "                 .batch(VALIDATION_SET_SIZE)\n",
    "                 .prefetch(buffer_size=AUTOTUNE)\n",
    "          )\n",
    "\n",
    "ds_patch_validation_with_labels = (ds_validation_paths\n",
    "                 .map(mappable_fn_patch, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .map(mappable_get_random_patch)\n",
    "                 .map(gan_rescale)\n",
    "                 .batch(VALIDATION_SET_SIZE)\n",
    "                 .prefetch(buffer_size=AUTOTUNE)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(autoencoder, test_data):\n",
    "    shp = test_data.shape\n",
    "    scores = []\n",
    "    for test_sample in test_data:\n",
    "        test_sample = tf.reshape(test_sample, [1, shp[1], shp[2], shp[3]])\n",
    "        score = autoencoder.evaluate(test_sample, test_sample, verbose=0)\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def predict_new(autoencoder, data):\n",
    "    predictions = autoencoder.predict(data)\n",
    "    mse = tf.keras.losses.mean_squared_error(predictions, data)\n",
    "\n",
    "    mean_loss_per_image = tf.reduce_mean(mse, axis=[1,2])\n",
    "    return mean_loss_per_image\n",
    "\n",
    "def mean_squared_error_w(y_true, y_pred):\n",
    "    if not K.is_tensor(y_pred):\n",
    "        y_pred = K.constant(y_pred)\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "    mses = K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "    std_of_mses = K.std(mses, axis=[1,2])\n",
    "    const = K.mean(mses, axis = [1,2]) + (__c * std_of_mses)\n",
    "    mask = K.cast(K.less(mses, const), dtype = \"float32\")\n",
    "    return mask * mses\n",
    "\n",
    "\n",
    "def visualize_ae_result(show_id, model, dump_path=None):\n",
    "    image_batch, label_batch = next(iter(ds_patch_validation_with_labels))\n",
    "    prediction = model.predict(tf.reshape(image_batch[show_id], [1, PATCH_SIZE,PATCH_SIZE,1]))\n",
    "\n",
    "    f, axarr = plt.subplots(1,2,figsize=(10,10))\n",
    "    axarr[0].imshow(image_batch[show_id])\n",
    "    axarr[1].imshow(prediction[0])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if dump_path is not None:\n",
    "        plt.savefig(dump_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, validation_data, validation_labels, autoencoder, dump_frequency=2):\n",
    "        self.validation_data = validation_data\n",
    "        self.validation_labels = validation_labels\n",
    "        self.autoencoder = autoencoder\n",
    "        self.real_reconstruction_error_history = []\n",
    "        self.fake_reconstruction_error_history = []\n",
    "        self.APCER_history = []\n",
    "        self.BCER_history = []\n",
    "        self.ACR_history = []\n",
    "        self.dump_frequency = dump_frequency\n",
    "      \n",
    "    \n",
    "    def dump_stats(self, file):\n",
    "        history_dump = {\n",
    "            'real_reconstruction_error': self.real_reconstruction_error_history,\n",
    "            'fake_reconstruction_error': self.fake_reconstruction_error_history,\n",
    "            'APCER': self.APCER_history,\n",
    "            'BCER': self.BCER_history,\n",
    "            'ACR': self.ACR_history,\n",
    "            'dump_frequency': self.dump_frequency\n",
    "            }\n",
    "\n",
    "        with open(file, 'w') as outfile:\n",
    "                json.dump(history_dump, outfile)   \n",
    "\n",
    "                \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if epoch%self.dump_frequency == 0:\n",
    "            visualize_ae_result(100, self.autoencoder)\n",
    "            predictions = predict(self.autoencoder, self.validation_data)\n",
    "            \n",
    "            real_reconstruction_error = sum(predictions[128:])/128\n",
    "            fake_reconstruction_error = sum(predictions[:128])/128\n",
    "            print('\\nrecunstruction error real: {} fake: {}'.format(real_reconstruction_error, fake_reconstruction_error))\n",
    "            \n",
    "            classification_threshold = (fake_reconstruction_error+real_reconstruction_error)/2\n",
    "\n",
    "            custom_threshhold_predictions = tf.cast(tf.math.less(\n",
    "                predictions, classification_threshold, name=None\n",
    "            ), dtype=tf.int32)\n",
    "\n",
    "            matrix = tf.math.confusion_matrix(\n",
    "                self.validation_labels, tf.math.round(custom_threshhold_predictions, name=None), num_classes=None, weights=None, dtype=tf.dtypes.int32,\n",
    "                name=None\n",
    "                )\n",
    "\n",
    "            print(matrix)\n",
    "            \n",
    "\n",
    "            APCER_count = int(matrix[0][1])\n",
    "            BPCER_count = int(matrix[1][0])\n",
    "            APCER_SFNR = APCER_count / (APCER_count + int(matrix[0][0]))\n",
    "            BPCER_SFPR = BPCER_count / (BPCER_count + int(matrix[1][1]))\n",
    "            ACR = (APCER_SFNR + BPCER_SFPR) / 2\n",
    "\n",
    "            print(\"APCER: \" + str(APCER_SFNR) + \" BPCER: \" + str(BPCER_SFPR) + \" ACR: \" + str(ACR))\n",
    "            \n",
    "            self.real_reconstruction_error_history.append(real_reconstruction_error)\n",
    "            self.fake_reconstruction_error_history.append(fake_reconstruction_error)\n",
    "            self.APCER_history.append(APCER_SFNR)\n",
    "            self.BCER_history.append(BPCER_SFPR)\n",
    "            self.ACR_history.append(ACR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_data, validation_labels = next(iter(ds_patch_validation_with_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init = RandomNormal(stddev=0.02)\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(4*4*1024, use_bias = False, input_shape = (200,), kernel_initializer=init))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Reshape((4, 4, 1024)))\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(512, (5, 5), strides = (2,2), padding = \"same\", use_bias = False, kernel_initializer=init))\n",
    "    assert model.output_shape == (None, 8, 8, 512)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(256, (5,5), strides = (2,2), padding = 'same', use_bias = False, kernel_initializer=init))\n",
    "    assert model.output_shape == (None, 16, 16, 256)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(128, (5,5), strides = (2,2), padding = 'same', use_bias = False, kernel_initializer=init))\n",
    "    assert model.output_shape == (None, 32, 32, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(1, (5,5), strides = (2,2), padding = 'same', use_bias = False, activation = 'tanh', kernel_initializer=init))\n",
    "    assert model.output_shape == (None, 64, 64, 1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[64, 64, 1], kernel_initializer=init))\n",
    "    assert model.output_shape == (None, 32, 32, 64)\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', kernel_initializer=init))\n",
    "    assert model.output_shape == (None, 16, 16, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same', kernel_initializer=init))\n",
    "    assert model.output_shape == (None, 8, 8, 256)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same', kernel_initializer=init))\n",
    "    assert model.output_shape == (None, 4, 4, 512)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.load_weights('./disc/')\n",
    "discriminator_base = tf.keras.Sequential(discriminator.layers[:-2])\n",
    "discriminator_base.trainable = True\n",
    "discriminator_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape = (64,64,1))\n",
    "x = discriminator_base(inputs, training = False)\n",
    "x = layers.Conv2D(1024, (5, 5), padding='same', kernel_initializer=init)(x) #latent vector\n",
    "discriminator_new = keras.Model(inputs, x)\n",
    "discriminator_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "generator.load_weights('./gen/')\n",
    "generator_base = tf.keras.Sequential(generator.layers[4:])\n",
    "generator_base.trainable = True\n",
    "\n",
    "inputs = keras.Input(shape = (4, 4, 1024))\n",
    "x = generator_base(inputs, training = False)\n",
    "generator_new = keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan_ae = tf.keras.models.Sequential([discriminator_new, generator_new])\n",
    "gan_ae.compile(optimizer=tf.keras.optimizers.Adam(0.00001, beta_1=0.5), loss='mean_squared_error')\n",
    "validationCallback = ValidationCallback(validation_data, validation_labels, gan_ae, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = gan_ae.fit(ds_patch_train,\n",
    "                          epochs=6000,\n",
    "                          callbacks=[validationCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan_ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_to_store = gan_ae\n",
    "model_path = './ae_run_1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.mkdir(model_path)\n",
    "\n",
    "with open(model_path + 'architecture.txt', 'w') as f:\n",
    "    with redirect_stdout(f): \n",
    "        print(model_to_store.summary())   \n",
    "        print('Batch_size:' + str(32))\n",
    "        print('Optimizer:' + 'adam 0.00001')\n",
    "        print('Loss:' + 'mse')\n",
    "        \n",
    "        \n",
    "visualize_ae_result(100, model_to_store, model_path + 'plot1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validationCallback.dump_stats(model_path + 'train_log')\n",
    "model_to_store.save_weights(model_path + 'weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_to_store.save(model_path + 'saved_model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(model_path + 'train_log') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "plt.plot(data['APCER'], label=\"APCER\")\n",
    "plt.plot(data['BCER'], label=\"BCER\")\n",
    "plt.plot(data['ACR'], label=\"ACR\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(data['real_reconstruction_error'], label=\"real\")\n",
    "plt.plot(data['fake_reconstruction_error'], label=\"fake\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(data['real_reconstruction_error'][:300], label=\"Bona Fide\")\n",
    "plt.plot(data['fake_reconstruction_error'][:300], label=\"Presentation Attack\")\n",
    "plt.ylim([0,0.15])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('training_progres')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
